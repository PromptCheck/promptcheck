version: 0.1
phases:
  - name: "Phase 0: Initial Setup & Planning"
    tasks:
      - name: "Clone repository (`https://github.com/b00gn1sh/promptcheck/`)"
        status: "done"
      - name: "Initial review of `README.md` (Project Blueprint)"
        status: "done"
      - name: "Create detailed task list in `README.md` (this task)"
        status: "done"
      - name: "Define initial project directory structure"
        status: "done"
      - name: "Set up virtual environment and install core dependencies (Python, Typer)"
        status: "done"
      - name: "Initialize `pyproject.toml` (or `setup.py`/`requirements.txt`)"
        status: "done"
      - name: "Grab `promptcheck` on PyPI and npm if available. (Appears available - USER TO ACTION)"
        status: "to-do"
        notes: "Appears available. User to register."
      - name: "Design initial logo/mascot sketch (mini raccoon engineer). (On hold - for end of prototyping)"
        status: "on-hold"
        notes: "Defer to end of prototyping phase."
      - name: "Set up GitHub Projects for Kanban board (mirroring tasks here). (On hold - README is primary tracker for now)"
        status: "on-hold"
        notes: "Decision made to use PROJECT_TASKS.yaml as primary for now."

  - name: "Phase 1: MVP (Core CLI & GitHub Action)"
    sub_phases:
      - name: "CLI - `promptcheck` (Python + Typer)"
        tasks:
          - name: "Switch to src/promptcheck/ layout."
            status: "done"
            notes: "Moved package to src/ and updated pyproject.toml."
          - name: "Optional-dependency split for NLTK (BLEU metric)."
            status: "done"
            notes: "NLTK moved to 'bleu' extra. Metrics code handles optional import."
          - name: "`promptcheck init` command"
            sub_tasks:
              - name: "Scaffold example `tests/*.yaml` files."
                status: "done"
              - name: "Scaffold example `promptcheck.config.yaml` (for API keys, default model, thresholds)."
                status: "done"
          - name: "Test Specification (`tests/*.yaml`)"
            sub_tasks:
              - name: "Finalize YAML structure (`name`, `description`, `type`, `input_source`, `input_data`, `expected_output`, `metric_configs`, `model_config`)."
                status: "done"
              - name: "Implement YAML parser."
                status: "done"
          - name: "`promptcheck run` command"
            sub_tasks:
              - name: "Core test execution engine."
                status: "done"
              - name: "Load and validate `promptcheck.config.yaml`."
                status: "done"
              - name: "LLM Provider Integration"
                sub_tasks:
                  - name: "Abstract provider interface."
                    status: "done"
                  - name: "OpenAI provider implementation."
                    status: "done"
                    notes: "Retry/timeout logic added. Checks env for OPENAI_API_KEY."
                  - name: "Groq provider implementation."
                    status: "done"
                    notes: "Retry/timeout logic added. Checks env for GROQ_API_KEY."
                  - name: "OpenRouter provider implementation."
                    status: "done"
                    notes: "Retry/timeout logic added. Checks env for OPENROUTER_API_KEY. Handles X-OpenRouter-Cost header."
              - name: "Metric Calculation"
                sub_tasks:
                  - name: "Exact match metric."
                    status: "done"
                  - name: "Regex match metric."
                    status: "done"
                  - name: "Rouge-L / BLEU-4 metric (using `rouge-score` or similar)."
                    status: "done"
                  - name: "Token count metric (prompt, completion)."
                    status: "done"
                  - name: "Latency metric (p95)."
                    status: "done"
                    notes: "Individual latency reporting complete. P95 for aggregate reporting later."
                  - name: "Cost calculation metric (based on token counts and provider pricing)."
                    status: "done"
                    notes: "Basic implementation with placeholder pricing."
                  - name: "(Stretch MVP) LLM-evaluate hallucination score (e.g., using a separate model)."
                    status: "to-do"
              - name: "Output `run.json` artifact with detailed results for each test (inputs, outputs, scores, pass/fail, cost, latency)."
                status: "done"
          - name: "`promptcheck report` command"
            sub_tasks:
              - name: "Read `run.json`."
                status: "to-do"
              - name: "Display a pretty summary table in the console."
                status: "to-do"
              - name: "(Stretch MVP) Generate a simple HTML report."
                status: "to-do"
          - name: "Error Handling & Logging"
            sub_tasks:
              - name: "Implement robust error handling for CLI operations."
                status: "to-do"
              - name: "Implement logging with configurable verbosity (with Trash-Panda humor)."
                status: "to-do"
          - name: "Unit & Integration Tests"
            sub_tasks:
              - name: "Write unit tests for metric calculations."
                status: "to-do"
              - name: "Write unit tests for CLI command parsing and execution."
                status: "to_do"
              - name: "Write integration tests for `promptcheck run` with mock LLM providers."
                status: "to-do"
              - name: "Create real sample test (`tests/basic_example.yaml`) that passes."
                status: "done"
                notes: |
                  Uses OpenRouter free tier model (mistralai/mistral-7b-instruct).
                  Prerequisite smoke-test from Execution Playbook v0.2 now passes with this test case,
                  verifying core CLI functionality, config/test loading, OpenRouter integration,
                  and calculations for regex_match, token_count, latency, and cost metrics.
                  JSON output also verified.

      - name: "GitHub Action (`promptcheck/action@v1`)"
        tasks:
          - name: "Create `action.yml` definition."
            status: "done"
          - name: "Develop Docker container environment (`python:slim` + dependencies)."
            status: "done"
            notes: "Dockerfile created with entrypoint.sh."
          - name: "Action script"
            sub_tasks:
              - name: "Checkout repository."
                status: "done"
                notes: "Handled by standard actions/checkout@v4 in example workflow."
              - name: "Setup Python and install `promptcheck` CLI."
                status: "done"
                notes: "Handled by Dockerfile and poetry install."
              - name: "Run `promptcheck run` (configurable via action inputs)."
                status: "done"
                notes: "Handled by entrypoint.sh using inputs."
              - name: "Upload `run.json` as a GitHub build artifact."
                status: "done"
                notes: "Implemented in example workflow using actions/upload-artifact@v4."
              - name: "Generate Markdown summary from `run.json`."
                status: "to-do"
          - name: "Post Markdown summary as a comment on the PR."
            status: "done"
            notes: "Implemented via post_comment.py script using gh CLI. Successfully posts to PRs."
          - name: "Fail action if evaluation metrics breach configurable thresholds."
            status: "done"
            notes: "CLI exits with non-zero code on failure; action entrypoint propagates this. Currently uses --soft-fail in PR check workflow, but core CLI supports hard fail."
          - name: "(S4 Follow-up) Implement PR check failure based on promptcheck run results (not just --soft-fail)."
            status: "to-do"
            notes: "Enhance CI workflow or post_comment.py to make the PR check fail if significant test regressions occur."
          - name: "(S4 Follow-up) PR comment bot: Update existing comment instead of posting new ones."
            status: "to-do"
            notes: "Investigate using gh CLI or GitHub API to find and update previous bot comments on a PR."
          - name: "(S4 Follow-up) Review/update source of promptcheck package for PR checks (local vs. TestPyPI/PyPI)."
            status: "to-do"
            notes: "Currently PR check job installs from TestPyPI. Consider changing to build/install from PR source for true branch testing."
          - name: "Documentation for the GitHub Action (in README and Marketplace)."
            status: "to-do"
          - name: "Create a sample repository demonstrating Action usage."
            status: "to-do"
            notes: "Current .github/workflows/eval.yml serves as an initial self-test/demo."

      - name: "Docs & Demo (MVP)"
        tasks:
          - name: "Detailed Quickstart in `README.md` for CLI."
            status: "to-do"
          - name: "Detailed Quickstart in `README.md` for GitHub Action."
            status: "to-do"
          - name: "Create a Loom video (or similar) for a quick demo."
            status: "to-do"
          - name: "(Stretch MVP) Basic landing page for the hosted dashboard."
            status: "to-do"

  - name: "Phase 2: V1 (Weeks 5-10)"
    tasks:
      - name: "Notifications"
        sub_tasks:
          - name: "Slack notifications (run completion, failures, summaries)."
            status: "to-do"
          - name: "Discord notifications."
            status: "to-do"
      - name: "Team & Project Features (Dashboard)"
        sub_tasks:
          - name: "Team member invitations and roles (admin, member)."
            status: "to-do"
          - name: "Project quotas and usage tracking."
            status: "to-do"
      - name: "Enhanced Integrations"
        sub_tasks:
          - name: "Webhook ingest for `run.json` from non-GitHub CI systems."
            status: "to-do"
      - name: "API & Security"
        sub_tasks:
          - name: "API key rotation."
            status: "to-do"
          - name: "Usage caps for API keys."
            status: "to-do"
      - name: "Billing (Stripe)"
        sub_tasks:
          - name: "Integrate Stripe for metered usage (e.g., tokens stored/processed beyond free tier)."
            status: "to-do"
          - name: "Integrate Stripe for Pro plan subscriptions (\\$9/mo)."
            status: "to-do"
          - name: "Implement logic for different plan tiers (Free, Pro, Agency)."
            status: "to-do"
      - name: "Extensibility"
        sub_tasks:
          - name: "Formalize plugin architecture for custom metrics."
            status: "to-do"
          - name: "Formalize plugin architecture for custom LLM providers."
            status: "to-do"
      - name: "Marketing & Community"
        sub_tasks:
          - name: "Blog post for launch."
            status: "to-do"
          - name: "\"Show HN\" post."
            status: "to-do"
          - name: "Posts on r/MachineLearning, IndieHackers, X."
            status: "to-do"
          - name: "Create \"Prompt Regression Checklist\" PDF lead magnet."
            status: "to-do"
      - name: "Polish & Performance"
        sub_tasks:
          - name: "Address feedback from beta users."
            status: "to-do"
          - name: "CLI performance optimizations (target overhead â‰¤ 5s per 10 tests)."
            status: "to-do"
          - name: "CLI binary size optimization (< 15MB goal)."
            status: "to-do"
          - name: "Documentation v1."
            status: "to-do"
          - name: "Official logo and branding assets."
            status: "to-do"

  - name: "Future Ideas & Backlog (Post-V1 / Post-MVP)"
    notes: "Items deferred from initial MVP or planned for later significant versions."
    tasks:
      - name: "(Stretch MVP) LLM-evaluate hallucination score (e.g., using a separate model)."
        status: "to-do"
        original_phase: "Phase 1 - Metric Calculation"
      - name: "(Stretch MVP) Generate a simple HTML report for `promptcheck report`."
        status: "to-do"
        original_phase: "Phase 1 - promptcheck report command"
      - name: "(Stretch MVP) Basic landing page for the hosted dashboard."
        status: "to-do"
        original_phase: "Phase 1 - Docs & Demo (MVP)"
      
      - name: "Async Runner for Concurrent LLM Calls"
        status: "to-do"
        notes: "Performance enhancement for `promptcheck run`."
        original_phase: "Future - Performance"

      - name: "Hosted Dashboard (Full Features - Post-Beta)"
        notes: "All tasks related to the full hosted dashboard beyond a potential beta/stub."
        # Tasks originally under 'Hosted Dashboard (Beta - FastAPI, Supabase, Next.js)' from Phase 1
        # can be considered part of this larger future effort if not stubbed for MVP.
        # For clarity, let's assume the detailed dashboard tasks are for this future phase.
        sub_phases: 
          - name: "Backend (FastAPI + Supabase/Postgres + Prisma)"
            tasks:
              - name: "Set up Supabase project."
                status: "to-do"
              - name: "Define database schema (users, projects, runs, results) using Prisma. Run migrations."
                status: "to-do"
              - name: "Implement email magic-link authentication (Supabase Auth)."
                status: "to-do"
              - name: "API endpoint to ingest `run.json` (secured by API key per project)."
                status: "to-do"
              - name: "API endpoints to query runs and results for a project."
                status: "to-do"
              - name: "Implement Row-Level Security (RLS) in Supabase."
                status: "to-do"
          - name: "Frontend (Next.js + shadcn/ui)"
            tasks:
              - name: "Basic project setup."
                status: "to-do"
              - name: "User authentication pages (login via magic link)."
                status: "to-do"
              - name: "Dashboard page"
                sub_tasks:
                  - name: "List projects for the user."
                    status: "to-do"
                  - name: "For a selected project, display a list of runs."
                    status: "to-do"
                  - name: "For a selected run, display detailed results."
                    status: "to-do"
                  - name: "Simple chart: quality (e.g., pass % or avg score) and cost over commits/time."
                    status: "to-do"
          - name: "Deployment & Ops (Dashboard)"
            tasks:
              - name: "Deploy FastAPI backend (e.g., Fly.io, Railway)."
                status: "to-do"
              - name: "Deploy Next.js frontend (e.g., Vercel, Fly.io, Railway)."
                status: "to-do"
          - name: "Security (Dashboard)"
            tasks:
              - name: "Ensure HTTPS/TLS for all communications."
                status: "to-do"
              - name: "JWT for API authentication."
                status: "to-do"

      # Other items from original "Future Ideas (Post-V1)" can remain here or be merged
      - name: "FlowLens DevTools Panel."
        status: "to-do"
      - name: "White-label OEM licensing."
        status: "to-do"
      - name: "VS Code extension."
        status: "to-do"
      - name: "Cursor IDE extension."
        status: "to-do"
      - name: "Auto-generation of failing regression prompts."
        status: "to-do"
      - name: "(Polish) Address Pydantic V1 style validator warnings in schemas.py."
        status: "done"
        notes: "Migrated class Config to model_config = ConfigDict and @validator to @model_validator."
      - name: "(Polish) Address PytestCollectionWarning for TestCase schema."
        status: "to-do"
        notes: "Low priority. Investigate pytest config to ignore Pydantic models or rename files if necessary."
      - name: "(Docs) Add note about fork PR permissions for GH_TOKEN in CI Action comment posting."
        status: "to-do"
        notes: "Clarify potential limitations for external contributors." 